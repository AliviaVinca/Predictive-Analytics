# -*- coding: utf-8 -*-
"""Copy of notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ozJ97VaQ-2eoOiAw921xImo_NaxbTyG4

# First Project : Predictive Analytics

Nama : Alivia Vinca Kustaryono

Cohort ID : MC006D5X2041

Email : mc006d5x2041@student.devacademy.id

# Data Loading

## Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve, auc
from sklearn.pipeline import Pipeline
import joblib

!pip install catboost

"""## Import Dataset"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

# Download kaggle dataset and unzip the file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d litvinenko630/insurance-claims

!unzip insurance-claims.zip

insurance = pd.read_csv('Insurance claims data.csv')
insurance.head(20)

"""# Exploratory Data Analysis (EDA)"""

# melihat tipe data dan kolom data
insurance.info()

# Hitung Claim Status
jumlah = insurance['claim_status'].value_counts()
persentase = insurance['claim_status'].value_counts(normalize=True) * 100
hasil = pd.DataFrame({'Jumlah': jumlah, 'Persentase (%)': persentase.round(2)})
print(hasil)

# melihat data duplikat
insurance.duplicated().sum()

# melihat data hilang
insurance.isnull().sum()

# melihat fitur kategorik
insurance.describe()

"""### Visualisasi & Korelasi Data"""

#Korelasi Data Numerik
# Ambil hanya kolom numerik
numeric_cols = insurance.select_dtypes(include=[np.number]).columns

# Hitung korelasi
corr_matrix = insurance[numeric_cols].corr()

#Heatmap Korelasi
plt.figure(figsize=(20, 8))
sns.heatmap(corr_matrix, annot=True, cmap='flare', fmt=".2f", linewidths=0.5)
plt.title('Heatmap Korelasi Antar Variabel Numerik')
plt.tight_layout()
plt.show()

# Heatmap Korelasi Setelah Preprocessing Data
# Pilih fitur numerik
numerical_features = insurance.select_dtypes(include=['int64', 'float64'])

# Korelasi
plt.figure(figsize=(14, 10))
sns.heatmap(numerical_features.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Korelasi Antar Fitur Numerik")
plt.tight_layout()
plt.show()

# Korelasi semua fitur numerik dengan target
target_corr = corr_matrix['claim_status'].drop('claim_status').sort_values(ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=target_corr.values, y=target_corr.index, palette='viridis')
plt.title('Korelasi Setiap Fitur Numerik dengan Target (claim_status)')
plt.xlabel('Korelasi')
plt.ylabel('Fitur')
plt.tight_layout()
plt.show()

# Barplot Distribusi Fitur yang Berpengaruh
categorical_sample = ['vehicle_age', 'customer_age','region_density', 'airbags', 'ncap_rating', 'claim_status']

fig, axes = plt.subplots(3, 2, figsize=(12, 10))

for ax, col in zip(axes.flatten(), categorical_sample):
    sns.countplot(data=insurance, x=col, ax=ax, palette='Set2')
    ax.set_title(f'Distribusi Fitur: {col}')
    ax.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Barplot Distribusi Fraud dan Not Fraud
plt.figure(figsize=(6, 4))
sns.countplot(x='claim_status', data=insurance, palette='Set2')
plt.title('Distribusi Klaim Asuransi (Barplot)')
plt.xlabel('Claim Status')
plt.ylabel('Jumlah')
plt.xticks([0, 1], ['Not Fraud (0)', 'Fraud (1)'])
plt.tight_layout()
plt.show()

# Pie chart Distribusi Fraud dan Not Fraud
claim_counts = insurance['claim_status'].value_counts()
labels = ['Not Fraud (0)', 'Fraud (1)']

plt.figure(figsize=(6, 6))
plt.pie(claim_counts, labels=labels, autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#ff6666'])
plt.title('Distribusi Klaim Asuransi (Pie Chart)')
plt.axis('equal')
plt.tight_layout()
plt.show()

"""# Preparation

## Drop Kolom
"""

# hapus kolom yang tidak digunakan
insurance.drop(['policy_id', 'segment', 'model', 'max_torque', 'max_power'], axis=1, inplace=True)

# cek kembali tipe data
insurance.info()

"""## Proses Encoding"""

#Pisahkan Binary dan Multi-Kategori
# Kolom kategorikal binary (nilai: Yes/No atau hanya 2 kategori)
binary_cols = [
    'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors', 'is_parking_camera',
    'rear_brakes_type', 'transmission_type','steering_type',
    'is_front_fog_lights', 'is_rear_window_wiper', 'is_rear_window_washer', 'is_rear_window_defogger',
    'is_brake_assist', 'is_power_door_locks', 'is_central_locking', 'is_power_steering',
    'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror', 'is_ecw', 'is_speed_alert'
]

# Label Encoding untuk binary features
le = LabelEncoder()
for col in binary_cols:
    insurance[col] = le.fit_transform(insurance[col])

#One-Hot Encoding untuk kategori Nominal
# Kolom kategorikal multi-kategori (bukan binary)
multi_cat_cols = [
    'region_code', 'fuel_type', 'engine_type'
]

# Gunakan One-Hot Encoding
insurance = pd.get_dummies(insurance, columns=multi_cat_cols, drop_first=True)

#Cek hasil encoding
print(insurance[binary_cols].head())
print(insurance.filter(like='segment_').head())

# cek kembali dataset yang sudah di preprocessing
insurance.head()

"""## Feature Scaling"""

# Fitur numerik yang akan discale
numerical_cols = [
    'subscription_length', 'vehicle_age', 'customer_age', 'region_density',
    'airbags', 'displacement', 'cylinder', 'turning_radius',
    'length', 'width', 'gross_weight', 'ncap_rating'
]

# Inisialisasi scaler
scaler = StandardScaler()

# Fit dan transform kolom numerik
insurance[numerical_cols] = scaler.fit_transform(insurance[numerical_cols])

# cek kembali fitur numerik untuk kolom numerikal
print(insurance[numerical_cols].describe())

#cek tipe data setelah preprocessing
insurance.info()

"""## Train Test Split"""

# Pisahkan fitur dan target
X = insurance.drop(columns='claim_status')
y = insurance['claim_status']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

#print data latih dan data uji juga distribusi y_train
print(f"Jumlah data latih: {X_train.shape[0]}")
print(f"Jumlah data uji: {X_test.shape[0]}")
print(f"Distribusi label y_train:\n{y_train.value_counts(normalize=True)}")

"""## SMOTE"""

# Inisialisasi SMOTE
smote = SMOTE(random_state=42)

# Terapkan SMOTE hanya pada data latih
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi setelah SMOTE
print(y_train_smote.value_counts(normalize=True))
print(f"Jumlah data latih setelah SMOTE: {X_train_smote.shape[0]}")

"""# Modeling"""

models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "CatBoost": CatBoostClassifier(verbose=0, random_state=42)
}

results = []

"""# Evaluasi"""

def evaluate_model(model, name):
    model.fit(X_train_smote, y_train_smote)
    y_pred = model.predict(X_test)
    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]) if hasattr(model, "predict_proba") else None
    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc
    })

for name, model in models.items():
    evaluate_model(model, name)

results_df = pd.DataFrame(results)
print(results_df)
# print evaluasi dari modeling

"""# Tuning Model XGBoost"""

# Best model tuning (XGBoost)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}
grid = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='f1', cv=3, n_jobs=-1, verbose=0)
grid.fit(X_train_smote, y_train_smote)
best_model = grid.best_estimator_

# Evaluasi Tuning Model

# Prediksi pada data uji
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]  # untuk ROC AUC

# Hitung metrik evaluasi
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)
roc_auc = roc_auc_score(y_test, y_proba)

# Tampilkan hasil
print("Evaluasi XGBoost setelah tuning:")
print(f"Accuracy  : {accuracy:.6f}")
print(f"Precision : {precision:.6f}")
print(f"Recall    : {recall:.6f}")
print(f"F1 Score  : {f1:.6f}")
print(f"ROC AUC   : {roc_auc:.6f}")

"""# Menjawab Pertanyaan Bisnis"""

# Save Pipeline
pipeline = Pipeline([('model', best_model)])
joblib.dump(pipeline, 'fraud_detector.pkl')

"""### 1. Bagaimana memprediksi apakah sebuah klaim asuransi memiliki potensi risiko tinggi?

Dengan membangun model klasifikasi (claim_status: 0 atau 1) menggunakan data historis dan memprediksi apakah klaim baru tergolong risiko tinggi (1).
"""

# Predict New Data
def detect_claim_risk(input_data):
    model = joblib.load('fraud_detector.pkl')
    return model.predict(input_data)

new_claim = X_test.iloc[[0]]
pred = detect_claim_risk(new_claim)
print("Prediksi risiko klaim:", "Risiko Tinggi" if pred[0] == 1 else "Aman")

"""Dengan membangun model klasifikasi (claim_status: 0 atau 1) menggunakan data historis dan memprediksi apakah klaim baru tergolong risiko tinggi (1).

✅ Penjelasan: Dengan model yang sudah dilatih (misalnya XGBoost), kita bisa input data baru dan model akan memberikan prediksi klaim yang berisiko tinggi.

### 2. Apa saja faktor-faktor yang mempengaruhi risiko pada klaim asuransi?
"""

# Feature Importance
importances = best_model.feature_importances_
importance_df = pd.DataFrame({'Fitur': X_train.columns, 'Tingkat_Kepentingan': importances}).sort_values(by='Tingkat_Kepentingan', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=importance_df.head(20), x='Tingkat_Kepentingan', y='Fitur')
plt.title('Faktor-faktor yang Mempengaruhi Risiko Klaim Asuransi (XGBoost)')
plt.tight_layout()
plt.show()

"""Dengan mengevaluasi feature importance dari model seperti XGBoost atau Random Forest. Kita bisa melihat fitur mana (misalnya: vehicle_age, costumer_age, displacement, dll) yang paling berpengaruh dalam keputusan model.

### 3. Bagaimana mengotomatiskan proses deteksi klaim yang mencurigakan secara efisien?
"""

# Misal pipeline preprocessing dan model
pipeline = Pipeline(steps=[
    ('model', best_model)
])

# Simpan model ke file
joblib.dump(pipeline, 'fraud_detector.pkl')

# Fungsi otomatis deteksi klaim
def detect_claim_risk(input_data):
    model = joblib.load('fraud_detector.pkl')
    prediction = model.predict(input_data)
    return prediction

# Misal input data baru dari sistem
incoming_claim = X_test.iloc[[0]]
hasil = detect_claim_risk(incoming_claim)

print("Deteksi otomatis klaim: ", "Risiko Tinggi" if hasil[0] == 1 else "Aman")

"""Dengan menyimpan model dalam bentuk pipeline dan membuat fungsi untuk mendeteksi secara otomatis klaim masuk baru.

✅ Penjelasan: Ini bisa dipanggil dari API/web app untuk langsung mendeteksi apakah klaim mencurigakan atau tidak secara real time.
"""